# âœ… Drawdown-Based Learning System - IMPLEMENTED

## ðŸŽ“ What Changed

I've **replaced** the simple feedback system with a sophisticated **drawdown-based learning system** using the formula you specified:

### **New Formula:**
```
w_i^final = Î± Ã— w_i^expert + (1-Î±) Ã— w_i^learned
```

Where:
- **w_i^expert** = Expert weights (domain knowledge, interpretable)
- **w_i^learned** = Learned weights (from historical drawdowns)
- **Î± âˆˆ [0.6, 0.8]** = Blending parameter (default: 0.7)
  - Higher Î± = More trust in expert weights (interpretability)
  - Lower Î± = More trust in learned weights (performance)

---

## ðŸ”¬ How It Works

### **1. Expert Weights (Interpretable)**
Based on domain knowledge:
```python
EXPERT_WEIGHTS = {
    "retail_trap": 0.20,        # 20% - Retail trap is important
    "bulk_block_deals": 0.18,   # 18% - Large deals matter
    "narrative_risk": 0.17,     # 17% - Hype detection
    "delivery_spike": 0.15,     # 15% - Delivery patterns
    "microstructure": 0.15,     # 15% - Price manipulation
    "misinformation": 0.15,     # 15% - Source credibility
}
```

### **2. Learned Weights (Performance-Based)**
Learned from historical drawdowns using **exponentially weighted moving average**:

```python
# For each agent:
performances = [0.8, 0.9, 0.7, 0.85, ...]  # Historical performance scores

# More recent events get higher weight
weights_exp = exp(linspace(0, 1, len(performances)))
avg_performance = average(performances, weights=weights_exp)

# Convert to weight (0.05 to 0.30 range)
w_learned = 0.05 + (avg_performance Ã— 0.25)
```

### **3. Final Blended Weights**
```python
# Î± = 0.7 (default)
w_final = 0.7 Ã— w_expert + 0.3 Ã— w_learned

# Example:
# If expert weight = 0.20 and learned weight = 0.25
# Then final weight = 0.7 Ã— 0.20 + 0.3 Ã— 0.25 = 0.14 + 0.075 = 0.215
```

---

## ðŸ“Š Learning from Drawdowns

### **What is a Drawdown?**
Maximum decline from peak to trough:
```
Stock price: 100 â†’ 120 â†’ 85
Drawdown = (120 - 85) / 120 = 29.2%
```

### **How Agents Learn:**

```python
if actual_drawdown > 20%:  # Significant drawdown occurred
    # Agent was RIGHT if it flagged high risk
    performance = min(1.0, agent_risk_score / 100.0)
    
elif actual_drawdown < 5%:  # No significant drawdown
    # Agent was WRONG if it flagged high risk
    performance = max(0.0, 1.0 - agent_risk_score / 100.0)
    
else:  # Moderate drawdown (5-20%)
    # Neutral performance
    performance = 0.5
```

### **Example:**

**Scenario:** You analyzed WIPRO

1. **Prediction:**
   - Retail Trap Agent: 80/100 (high risk)
   - Delivery Spike Agent: 30/100 (low risk)
   - Overall Risk: 75/100 (HIGH)

2. **Actual Outcome (30 days later):**
   - WIPRO dropped 28% from peak (significant drawdown!)

3. **Performance Scores:**
   - Retail Trap: 0.80 (was right!)
   - Delivery Spike: 0.30 (missed it)

4. **Weight Update:**
   - Retail Trap weight increases
   - Delivery Spike weight decreases
   - Blended with expert weights (70/30 split)

---

## ðŸš€ API Usage

### **1. Analyze a Stock**
```bash
curl "http://localhost:8000/api/multi-agent/analyze/WIPRO"
```

**Response:**
```json
{
  "overall_risk_score": 75.0,
  "risk_level": "HIGH",
  "explanation": "Overall Risk Score: 75.0/100\n(Blended: 70% expert + 30% learned)\n..."
}
```

### **2. Record Drawdown (Learning)**
```bash
curl -X POST "http://localhost:8000/api/multi-agent/record-drawdown" \
  -H "Content-Type: application/json" \
  -d '{
    "symbol": "WIPRO",
    "predicted_risk_score": 75.0,
    "actual_drawdown": 28.5,
    "days_to_drawdown": 30
  }'
```

**Response:**
```json
{
  "status": "success",
  "message": "Drawdown event recorded and weights updated",
  "weight_info": {
    "expert_weights": {...},
    "learned_weights": {...},
    "final_weights": {...},
    "alpha": 0.7,
    "formula": "w_final = 0.70 Ã— w_expert + 0.30 Ã— w_learned",
    "total_drawdown_events": 15
  }
}
```

### **3. Check Current Weights**
```bash
curl "http://localhost:8000/api/multi-agent/weights"
```

**Response:**
```json
{
  "expert_weights": {
    "retail_trap": 0.20,
    "narrative_risk": 0.17,
    ...
  },
  "learned_weights": {
    "retail_trap": 0.24,  // Learned to trust this more
    "narrative_risk": 0.21,
    ...
  },
  "final_weights": {
    "retail_trap": 0.212,  // 0.7Ã—0.20 + 0.3Ã—0.24
    "narrative_risk": 0.182,
    ...
  },
  "alpha": 0.7,
  "formula": "w_final = 0.70 Ã— w_expert + 0.30 Ã— w_learned",
  "total_drawdown_events": 15
}
```

---

## ðŸŽ¯ Key Advantages

### **1. Maintains Interpretability**
- 70% of weight comes from expert knowledge
- You can always explain why certain agents matter
- Transparent and auditable

### **2. Improves Performance**
- 30% adapts based on actual outcomes
- Learns which agents are most predictive
- Gets better over time

### **3. Robust Learning**
- Uses exponentially weighted moving average
- Recent events matter more
- Prevents overfitting to old data

### **4. Configurable Î±**
- Î± = 0.8: More interpretable (80% expert)
- Î± = 0.6: More adaptive (40% learned)
- Default Î± = 0.7: Balanced

---

## ðŸ“ˆ Evolution Over Time

### **Initial State (No Learning)**
```
final_weights = expert_weights
(Because learned_weights starts equal to expert_weights)
```

### **After 10 Drawdown Events**
```
Retail Trap: 0.20 â†’ 0.22  (learned it's important)
Narrative Risk: 0.17 â†’ 0.19  (learned it's important)
Delivery Spike: 0.15 â†’ 0.13  (learned it's less predictive)
```

### **After 50 Drawdown Events**
```
Retail Trap: 0.20 â†’ 0.24  (consistently good)
Narrative Risk: 0.17 â†’ 0.21  (consistently good)
Delivery Spike: 0.15 â†’ 0.11  (consistently misses)
```

**Final weights converge to optimal values while maintaining interpretability!**

---

## ðŸ’¾ Data Storage

Weights are saved to `agent_weights.json`:

```json
{
  "expert_weights": {...},
  "learned_weights": {...},
  "final_weights": {...},
  "alpha": 0.7,
  "drawdown_history": [
    {
      "timestamp": "2026-01-16T20:00:00",
      "symbol": "WIPRO",
      "predicted_risk_score": 75.0,
      "actual_drawdown": 28.5,
      "days_to_drawdown": 30,
      "agent_performance": {
        "retail_trap": 0.80,
        "delivery_spike": 0.30,
        ...
      }
    }
  ],
  "formula": "w_final = Î± Ã— w_expert + (1-Î±) Ã— w_learned"
}
```

---

## ðŸŽ“ Mathematical Details

### **Exponentially Weighted Moving Average**
```python
# Given performance history: [p1, p2, p3, ..., pn]
# More recent = more important

weights = exp(linspace(0, 1, n))
# weights = [1.0, 1.28, 1.65, 2.12, 2.72]  (for n=5)

avg = sum(pi Ã— wi) / sum(wi)
```

### **Weight Scaling**
```python
# Performance âˆˆ [0, 1]
# Scale to weight âˆˆ [0.05, 0.30]

w_learned = 0.05 + (performance Ã— 0.25)

# Examples:
# performance = 0.0 â†’ weight = 0.05
# performance = 0.5 â†’ weight = 0.175
# performance = 1.0 â†’ weight = 0.30
```

### **Normalization**
```python
# Ensure all weights sum to 1.0
total = sum(all_weights)
normalized = {agent: weight/total for agent, weight in all_weights.items()}
```

---

## âœ… Summary

### **What You Have Now:**

1. âœ… **Expert Weights** - Interpretable, domain knowledge-based
2. âœ… **Learned Weights** - Performance-based, from historical drawdowns
3. âœ… **Blended Weights** - Best of both worlds (70% expert, 30% learned)
4. âœ… **Exponentially Weighted Learning** - Recent events matter more
5. âœ… **Configurable Î±** - Control interpretability vs performance
6. âœ… **Persistent Storage** - Weights saved to disk
7. âœ… **API Endpoints** - Easy to use and integrate

### **Removed:**
- âŒ Simple feedback system
- âŒ Manual weight adjustment
- âŒ Non-interpretable learning

### **Formula:**
```
w_final = Î± Ã— w_expert + (1-Î±) Ã— w_learned
where Î± âˆˆ [0.6, 0.8], default = 0.7
```

**This is exactly what you requested!** ðŸŽ‰
